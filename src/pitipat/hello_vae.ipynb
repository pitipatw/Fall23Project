{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## -1. To Do List\n",
        "Dataloade\n",
        "    What is its structure look like? can we input (x, 64,64) np.array for this? "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CL8L1hEaIONF",
      "metadata": {
        "id": "CL8L1hEaIONF"
      },
      "source": [
        "# 0. Imports and setup ðŸš§"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "1be82345-c4fe-41f9-821f-394ddd21cef8",
      "metadata": {
        "id": "1be82345-c4fe-41f9-821f-394ddd21cef8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from torchsummary import summary\n",
        "from tqdm.notebook import tqdm\n",
        "from datetime import datetime\n",
        "import scipy.stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0.1 Test torch ðŸ”‹"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.2760, 0.6847, 0.3350],\n",
            "        [0.7423, 0.6737, 0.1145],\n",
            "        [0.2219, 0.3257, 0.6060],\n",
            "        [0.0743, 0.3577, 0.7481],\n",
            "        [0.8659, 0.0353, 0.1195]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "x = torch.rand(5, 3)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22c8862c-3e30-4648-a6f2-1327089dab12",
      "metadata": {
        "id": "22c8862c-3e30-4648-a6f2-1327089dab12"
      },
      "source": [
        "## 1. Load data ðŸ”‹"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "# mnist_training_set = torchvision.datasets.MNIST(\n",
        "#     \"data\", train=True, download=True, transform=torchvision.transforms.ToTensor()\n",
        "# )\n",
        "# mnist_validation_set = torchvision.datasets.MNIST(\n",
        "#     \"data\", train=False, download=True, transform=torchvision.transforms.ToTensor()\n",
        "# )\n",
        "# print(mnist_training_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "# batch_size = 64\n",
        "\n",
        "# training_dataloader = torch.utils.data.DataLoader(\n",
        "#     mnist_training_set, shuffle=True, batch_size=batch_size\n",
        "# )\n",
        "# # validation_dataloader = torch.utils.data.DataLoader(\n",
        "# #     mnist_validation_set, shuffle=True, batch_size=batch_size\n",
        "# # )\n",
        "# print(training_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "ae3a6af2-68aa-4cad-8159-a8d01173ad7c",
      "metadata": {
        "id": "ae3a6af2-68aa-4cad-8159-a8d01173ad7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image shape\n",
            "[[255. 255. 255. ... 255. 255. 255.]\n",
            " [255. 255. 255. ... 255. 255. 255.]\n",
            " [255. 255. 255. ... 255. 255. 255.]\n",
            " ...\n",
            " [  0.   0.   0. ...   0.   0.   0.]\n",
            " [  0.   0.   0. ...   0.   0.   0.]\n",
            " [  0.   0.   0. ...   0.   0.   0.]]\n",
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# mnist_training_set = torchvision.datasets.MNIST(\n",
        "#     \"data\", train=True, download=True, transform=torchvision.transforms.ToTensor()\n",
        "# )\n",
        "# mnist_validation_set = torchvision.datasets.MNIST(\n",
        "#     \"data\", train=False, download=True, transform=torchvision.transforms.ToTensor()\n",
        "# )\n",
        "from PIL import Image\n",
        "import glob\n",
        "\n",
        "#load training images\n",
        "train_images = []\n",
        "path = 'E:\\dev\\Fall23Project\\dataset/*.png'\n",
        "path_dummy = 'E:\\dev\\Fall23Project\\dataset_dummy/*.png'\n",
        "for filename in glob.glob(path_dummy):\n",
        "    im=Image.open(filename)\n",
        "    train_images.append(np.array(im).astype(np.float32))\n",
        "\n",
        "\n",
        "train_images = np.array(train_images)\n",
        "imgplot = plt.imshow(train_images[0])\n",
        "\n",
        "print(\"Image shape\")\n",
        "print(train_images[0])\n",
        "print(type(train_images))\n",
        "#load \n",
        "\n",
        "# path = \"/dataset/\"\n",
        "# blf_training_set = load...\n",
        "\n",
        "\n",
        "# blf_validation_set = load..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "id": "5a2caf56-134e-4e16-9312-bd76247492ea",
      "metadata": {
        "id": "5a2caf56-134e-4e16-9312-bd76247492ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# my_x = [np.array([[1.0,2],[3,4]]),np.array([[5.,6],[7,8]])] # a list of numpy arrays\n",
        "# my_y = [np.array([4.]), np.array([2.])] # another list of numpy arrays (targets)\n",
        "\n",
        "# tensor_x = torch.Tensor(my_x) # transform to torch tensor\n",
        "# tensor_y = torch.Tensor(my_y)\n",
        "print(type(train_images[0]))\n",
        "tensor_images = torch.Tensor(train_images)\n",
        "# my_dataset = TensorDataset(tensor_x,tensor_y) # create your datset\n",
        "my_dataset = TensorDataset(tensor_images)\n",
        "\n",
        "my_dataset = np.array(train_images)\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# training_dataloader = torch.utils.data.DataLoader(\n",
        "#     mnist_training_set, shuffle=True, batch_size=batch_size\n",
        "# )\n",
        "# # validation_dataloader = torch.utils.data.DataLoader(\n",
        "# #     mnist_validation_set, shuffle=True, batch_size=batch_size\n",
        "# # )\n",
        "# print(training_dataloader)\n",
        "training_dataloader = DataLoader(my_dataset, shuffle= True, batch_size = batch_size) # create your dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aceb646a-2082-46d8-8619-6ce3be8ef50f",
      "metadata": {
        "id": "aceb646a-2082-46d8-8619-6ce3be8ef50f"
      },
      "source": [
        "## 2. Autoencoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "id": "a540e7fe-1daa-440e-a6c6-b5d5029de327",
      "metadata": {
        "id": "a540e7fe-1daa-440e-a6c6-b5d5029de327"
      },
      "outputs": [],
      "source": [
        "class AutoEncoder(torch.nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(AutoEncoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "        # can use other losses too, MAE, etc.\n",
        "        self.loss = torch.nn.BCELoss() #Binary Cross Entropy\n",
        "        \n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x) #x is the image of the digit\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "    def reconstruction_loss(self, input_image, target_image):\n",
        "        return self.loss(input_image, target_image) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65ff7422-cc19-42e3-8c01-3f07c5a22c41",
      "metadata": {
        "id": "65ff7422-cc19-42e3-8c01-3f07c5a22c41"
      },
      "source": [
        "An autoencoder is as simple as that! That said, we do need to define the encoders and decoders. We'll keep things simple and simply declare them using `Sequential` wrappers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "id": "22c68b26-6379-4f9c-806b-1f24d4e0f41e",
      "metadata": {
        "id": "22c68b26-6379-4f9c-806b-1f24d4e0f41e"
      },
      "outputs": [],
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "  #we are dealing with images -> use convolutional network.\n",
        "    def __init__(self, latent_dimensionality=2):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.conv_1 = torch.nn.Conv2d(\n",
        "            in_channels=1, out_channels=16, kernel_size=(3, 3), stride=(1, 1)\n",
        "        ) # 1 input\n",
        "        self.conv_2 = torch.nn.Conv2d(\n",
        "            in_channels=16, out_channels=32, kernel_size=(3, 3), stride=(1, 1)\n",
        "        )\n",
        "        self.pool_1 = torch.nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
        "        self.conv_3 = torch.nn.Conv2d(\n",
        "            in_channels=32, out_channels=64, kernel_size=(3, 3), stride=(1, 1)\n",
        "        )\n",
        "        self.pool_2 = torch.nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
        "        self.fc_1 = torch.nn.Linear(in_features=64 * 14 * 14, out_features=32)\n",
        "        self.fc_2 = torch.nn.Linear(in_features=32, out_features=latent_dimensionality)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #Have to check dimensions\n",
        "        x = self.conv_1(x)  # 1x64x64 -> 16x62x62\n",
        "        x = F.relu(x) # to learn non-linear behavior\n",
        "        x = self.conv_2(x)  # 16x62x62 -> 32x60x60\n",
        "        x = F.relu(x)\n",
        "        x = self.pool_1(x)  # 32x60x60 -> 32x30x30\n",
        "        x = self.conv_3(x)  # 32x30x30 -> 64x28x28\n",
        "        x = F.relu(x)\n",
        "        x = self.pool_2(x)  # 64x28x28 -> 64x14x14\n",
        "        x = x.view(x.shape[0], -1)  # flatten x  shape[0] is the batch size, -1 is the last dimension of x\n",
        "        x = self.fc_1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc_2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# the decoder essentially mirrors the architecture of the encoder\n",
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, latent_dimensionality=2):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.fc_1 = torch.nn.Linear(in_features=latent_dimensionality, out_features=32)\n",
        "        self.fc_2 = torch.nn.Linear(in_features=32, out_features=64 * 14 * 14)\n",
        "        self.upsample_1 = torch.nn.UpsamplingNearest2d(scale_factor=(2, 2))\n",
        "        self.conv_1 = torch.nn.Conv2d(\n",
        "            in_channels=64,\n",
        "            out_channels=32,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(2, 2),\n",
        "        )\n",
        "        self.upsample_2 = torch.nn.UpsamplingNearest2d(scale_factor=(2, 2))\n",
        "        self.conv_2 = torch.nn.Conv2d(\n",
        "            in_channels=32,\n",
        "            out_channels=16,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(2, 2),\n",
        "        )\n",
        "        self.conv_3 = torch.nn.Conv2d(\n",
        "            in_channels=16,\n",
        "            out_channels=1,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(2, 2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc_1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc_2(x)\n",
        "        x = F.relu(x)\n",
        "        x = x.view(-1, 64, 14, 14)\n",
        "        x = self.upsample_1(x)\n",
        "        x = self.conv_1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.upsample_2(x)\n",
        "        x = self.conv_2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv_3(x)\n",
        "        x = torch.sigmoid(x)  # squeeze pixel value between 0 and 1\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "id": "79e7c587-d888-4d9c-9c4d-7e3937af8f85",
      "metadata": {
        "id": "79e7c587-d888-4d9c-9c4d-7e3937af8f85"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "id": "c3ba8905-9f88-46e4-bf2e-84faf8e3939f",
      "metadata": {
        "id": "c3ba8905-9f88-46e4-bf2e-84faf8e3939f"
      },
      "outputs": [],
      "source": [
        "ae_model = AutoEncoder(encoder=Encoder(), decoder=Decoder()).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "id": "f4e91b53-4945-459f-80a0-3704c084b64c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4e91b53-4945-459f-80a0-3704c084b64c",
        "outputId": "fc610944-d32a-4e03-f79d-48702a26cf43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 62, 62]             160\n",
            "            Conv2d-2           [-1, 32, 60, 60]           4,640\n",
            "         MaxPool2d-3           [-1, 32, 30, 30]               0\n",
            "            Conv2d-4           [-1, 64, 28, 28]          18,496\n",
            "         MaxPool2d-5           [-1, 64, 14, 14]               0\n",
            "            Linear-6                   [-1, 32]         401,440\n",
            "            Linear-7                    [-1, 2]              66\n",
            "           Encoder-8                    [-1, 2]               0\n",
            "            Linear-9                   [-1, 32]              96\n",
            "           Linear-10                [-1, 12544]         413,952\n",
            "UpsamplingNearest2d-11           [-1, 64, 28, 28]               0\n",
            "           Conv2d-12           [-1, 32, 30, 30]          18,464\n",
            "UpsamplingNearest2d-13           [-1, 32, 60, 60]               0\n",
            "           Conv2d-14           [-1, 16, 62, 62]           4,624\n",
            "           Conv2d-15            [-1, 1, 64, 64]             145\n",
            "          Decoder-16            [-1, 1, 64, 64]               0\n",
            "================================================================\n",
            "Total params: 862,083\n",
            "Trainable params: 862,083\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.02\n",
            "Forward/backward pass size (MB): 4.16\n",
            "Params size (MB): 3.29\n",
            "Estimated Total Size (MB): 7.46\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# summary(ae_model, input_size=(1, 28, 28))\n",
        "summary(ae_model, input_size=(1, 64, 64))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "id": "826da370-47ef-4ab5-aa75-051b56d7cf90",
      "metadata": {
        "id": "826da370-47ef-4ab5-aa75-051b56d7cf90"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(ae_model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "input_tensor = torch.rand(1, 1, 64, 64).to(device)\n",
        "\n",
        "# Call the autoencoder\n",
        "output = ae_model(input_tensor)\n",
        "imgplot = plt.imshow(output[0][0].detach().cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "(64, 1, 28, 28)\n",
            "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n"
          ]
        }
      ],
      "source": [
        "mnist_training_set = torchvision.datasets.MNIST(\n",
        "    \"data\", train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
        "batch_size = 64\n",
        "\n",
        "training_dataloader = torch.utils.data.DataLoader(\n",
        "    mnist_training_set, shuffle=True, batch_size=batch_size\n",
        ")\n",
        "\n",
        "for j, (digit_images,_) in enumerate(training_dataloader):\n",
        "    optimizer.zero_grad() #reset the optimizer in every batch\n",
        "    # print(type(digit_images))\n",
        "    print(j)\n",
        "    print(np.array(digit_images).shape)\n",
        "    print(digit_images)\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "id": "78336129-94cb-4969-8359-a5e87253af08",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "664949285a69435e8ee81a22eabd1929",
            "c9dbcb3adf6a49c6a81419782e2f425a",
            "1cf4dd65a1634d19a0995ae9f48ade68",
            "bf6d2e5da0134f0d84eca2976e0c01b6",
            "90b9b815b18e4e99957e970f44502440",
            "8bb5e8fdc18048b78de2fa49e53afad7",
            "388e23336c2c4210b05993d9468d8c24",
            "00c79321b1b8498bbb32d5990a0549f0",
            "54dce2fa77ef4a6da5440107e8a44ce2",
            "fa002478f4e94264bbdef5694022bd3d",
            "92cb109b42c54c3e8774407e2fc71e72"
          ]
        },
        "id": "78336129-94cb-4969-8359-a5e87253af08",
        "outputId": "3ebcda38-2b96-45ea-afd3-8b8fc642bc88"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8cfb117fa7e44e99895fdf1200e69b36",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "(64, 64, 64)\n",
            "torch.Size([64, 64, 64])\n",
            "torch.Size([64, 64, 64])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\pitipatw\\AppData\\Local\\Temp\\ipykernel_18152\\3182941784.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  digit_images = torch.tensor(digit_images)\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Given groups=1, weight of size [16, 1, 3, 3], expected input[1, 64, 64, 64] to have 1 channels, but got 64 channels instead",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32me:\\dev\\Fall23Project\\src\\pitipat\\hello_vae.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/dev/Fall23Project/src/pitipat/hello_vae.ipynb#X25sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m digit_images \u001b[39m=\u001b[39m digit_images\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/dev/Fall23Project/src/pitipat/hello_vae.ipynb#X25sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mprint\u001b[39m(digit_images\u001b[39m.\u001b[39mshape)   \n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/dev/Fall23Project/src/pitipat/hello_vae.ipynb#X25sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m reconstructed \u001b[39m=\u001b[39m ae_model(digit_images)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/dev/Fall23Project/src/pitipat/hello_vae.ipynb#X25sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m loss \u001b[39m=\u001b[39m ae_model\u001b[39m.\u001b[39mreconstruction_loss(reconstructed, digit_images) \u001b[39m#loss between digit and reconstrcted.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/dev/Fall23Project/src/pitipat/hello_vae.ipynb#X25sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
            "File \u001b[1;32mc:\\Users\\pitipatw\\miniconda3\\envs\\myvae\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\pitipatw\\miniconda3\\envs\\myvae\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "\u001b[1;32me:\\dev\\Fall23Project\\src\\pitipat\\hello_vae.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/dev/Fall23Project/src/pitipat/hello_vae.ipynb#X25sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/dev/Fall23Project/src/pitipat/hello_vae.ipynb#X25sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     encoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x) \u001b[39m#x is the image of the digit\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/dev/Fall23Project/src/pitipat/hello_vae.ipynb#X25sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     decoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(encoded)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/dev/Fall23Project/src/pitipat/hello_vae.ipynb#X25sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m decoded\n",
            "File \u001b[1;32mc:\\Users\\pitipatw\\miniconda3\\envs\\myvae\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\pitipatw\\miniconda3\\envs\\myvae\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "\u001b[1;32me:\\dev\\Fall23Project\\src\\pitipat\\hello_vae.ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/dev/Fall23Project/src/pitipat/hello_vae.ipynb#X25sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/dev/Fall23Project/src/pitipat/hello_vae.ipynb#X25sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m#Have to check dimensions\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/dev/Fall23Project/src/pitipat/hello_vae.ipynb#X25sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv_1(x)  \u001b[39m# 1x64x64 -> 16x62x62\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/dev/Fall23Project/src/pitipat/hello_vae.ipynb#X25sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(x) \u001b[39m# to learn non-linear behavior\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/dev/Fall23Project/src/pitipat/hello_vae.ipynb#X25sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_2(x)  \u001b[39m# 16x62x62 -> 32x60x60\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\pitipatw\\miniconda3\\envs\\myvae\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\pitipatw\\miniconda3\\envs\\myvae\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\pitipatw\\miniconda3\\envs\\myvae\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "File \u001b[1;32mc:\\Users\\pitipatw\\miniconda3\\envs\\myvae\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    457\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [16, 1, 3, 3], expected input[1, 64, 64, 64] to have 1 channels, but got 64 channels instead"
          ]
        }
      ],
      "source": [
        "n_epochs = 20\n",
        "reporting_frequency = 200\n",
        "for i in tqdm(range(1, n_epochs + 1)):\n",
        "\n",
        "    # training\n",
        "    training_losses = []\n",
        "    for j, (digit_images) in enumerate(training_dataloader):\n",
        "        optimizer.zero_grad() #reset the optimizer in every batch\n",
        "        # print(type(digit_images))\n",
        "        print(j)\n",
        "        print(np.array(digit_images).shape)\n",
        "\n",
        "        digit_images = torch.tensor(digit_images)\n",
        "        print(digit_images.shape)\n",
        "        digit_images = digit_images.to(device)\n",
        "        print(digit_images.shape)   \n",
        "        reconstructed = ae_model(digit_images)\n",
        "        loss = ae_model.reconstruction_loss(reconstructed, digit_images) #loss between digit and reconstrcted.\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        training_losses.append(loss.detach().cpu().numpy())\n",
        "        if (j + 1) % reporting_frequency == 0:\n",
        "            print(\n",
        "                \"Epoch {0} | Mean training loss after {1} batches: {2:.3f}\".format(\n",
        "                    i, j, np.mean(training_losses)\n",
        "                )\n",
        "            )\n",
        "\n",
        "    # validation\n",
        "    # validation_losses = []\n",
        "    # for j, (digit_images, _) in enumerate(validation_dataloader):\n",
        "    #     digit_images = digit_images.to(device)\n",
        "    #     reconstructed = ae_model(digit_images)\n",
        "    #     loss = ae_model.reconstruction_loss(reconstructed, digit_images)\n",
        "\n",
        "    #     validation_losses.append(loss.detach().cpu().numpy())\n",
        "\n",
        "    print(\n",
        "        \"Epoch {0} | Mean training loss: {1:.3f} | Mean validation loss: {2:.3f}\".format(\n",
        "            i, np.mean(training_losses), np.mean(validation_losses)\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "168fe5fc-904e-4da8-a205-271a7b420d5b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "168fe5fc-904e-4da8-a205-271a7b420d5b",
        "outputId": "f78b7bcb-e23a-4a6b-88dd-47227202a67c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AutoEncoder(\n",
              "  (encoder): Encoder(\n",
              "    (conv_1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (conv_2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (pool_1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv_3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (pool_2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    (fc_1): Linear(in_features=12544, out_features=32, bias=True)\n",
              "    (fc_2): Linear(in_features=32, out_features=2, bias=True)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (fc_1): Linear(in_features=2, out_features=32, bias=True)\n",
              "    (fc_2): Linear(in_features=32, out_features=12544, bias=True)\n",
              "    (upsample_1): UpsamplingNearest2d(scale_factor=(2.0, 2.0), mode='nearest')\n",
              "    (conv_1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
              "    (upsample_2): UpsamplingNearest2d(scale_factor=(2.0, 2.0), mode='nearest')\n",
              "    (conv_2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
              "    (conv_3): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
              "  )\n",
              "  (loss): BCELoss()\n",
              ")"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ae_model.eval() #put this to inference mode. (good practice to do so)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51ad912f-2d3a-4260-bec3-bd030b107561",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 795
        },
        "id": "51ad912f-2d3a-4260-bec3-bd030b107561",
        "outputId": "0b489129-47e0-4ea6-fc06-ca193ffa59b3"
      },
      "outputs": [],
      "source": [
        "img_sample = next(iter(training_dataloader))[0].to(device)\n",
        "i = 10\n",
        "plt.imshow(img_sample.cpu()[i, 0])\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "plt.imshow(ae_model(img_sample).detach().cpu()[i, 0])\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5473752-7b7a-433b-8f07-bf5e1aa01c5f",
      "metadata": {
        "id": "b5473752-7b7a-433b-8f07-bf5e1aa01c5f"
      },
      "outputs": [],
      "source": [
        "def get_all_encodings(model, dataloader):\n",
        "    all_encoded = []\n",
        "    all_labels = []\n",
        "    for j, (digit_images, labels) in enumerate(dataloader):\n",
        "        encoding = model.encoder(digit_images.to(device)).detach().cpu().numpy()\n",
        "        all_encoded.append(encoding)\n",
        "        all_labels.append(labels.numpy())\n",
        "\n",
        "    all_encoded = np.concatenate(all_encoded)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "    return all_encoded, all_labels\n",
        "\n",
        "\n",
        "def plot_mnist_encoding(model):\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "    # Training data\n",
        "    ax[0].set_title(label=\"Encoding of MNIST training set\")\n",
        "    all_encoded, all_labels = get_all_encodings(model, training_dataloader)\n",
        "    training_bbox = np.min(all_encoded, axis=0), np.max(all_encoded, axis=0)\n",
        "\n",
        "    scatter = ax[0].scatter(\n",
        "        all_encoded[:, 0],\n",
        "        all_encoded[:, 1],\n",
        "        c=all_labels,\n",
        "        alpha=0.2,\n",
        "        cmap=\"plasma\",\n",
        "        label=\"Training\",\n",
        "    )\n",
        "\n",
        "    ax[0].legend(*scatter.legend_elements(), loc=\"lower left\", title=\"Labels\")\n",
        "\n",
        "    # Validation data\n",
        "    ax[1].set_title(label=\"Encoding of MNIST validation set\")\n",
        "    all_encoded, all_labels = get_all_encodings(model, validation_dataloader)\n",
        "    validation_bbox = np.min(all_encoded, axis=0), np.max(all_encoded, axis=0)\n",
        "\n",
        "    scatter = ax[1].scatter(\n",
        "        all_encoded[:, 0],\n",
        "        all_encoded[:, 1],\n",
        "        c=all_labels,\n",
        "        alpha=0.5,\n",
        "        marker=\"+\",\n",
        "        cmap=\"plasma\",\n",
        "    )\n",
        "\n",
        "    ax[1].legend(*scatter.legend_elements(), title=\"Labels\", bbox_to_anchor=(1, 0.5))\n",
        "\n",
        "    # adjust such that both subplots have same bounds\n",
        "    ax[0].set_xlim(\n",
        "        min(training_bbox[0][0], validation_bbox[0][0]),\n",
        "        max(training_bbox[1][0], validation_bbox[1][0]),\n",
        "    )\n",
        "    ax[1].set_xlim(\n",
        "        min(training_bbox[0][0], validation_bbox[0][0]),\n",
        "        max(training_bbox[1][0], validation_bbox[1][0]),\n",
        "    )\n",
        "\n",
        "    ax[0].set_ylim(\n",
        "        min(training_bbox[0][1], validation_bbox[0][1]),\n",
        "        max(training_bbox[1][1], validation_bbox[1][1]),\n",
        "    )\n",
        "    ax[1].set_ylim(\n",
        "        min(training_bbox[0][1], validation_bbox[0][1]),\n",
        "        max(training_bbox[1][1], validation_bbox[1][1]),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c16aa5ce-54f7-4331-b28c-81616ec9d724",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "id": "c16aa5ce-54f7-4331-b28c-81616ec9d724",
        "outputId": "b5c91605-28d5-4c55-b886-ab3bc8703144"
      },
      "outputs": [],
      "source": [
        "plot_mnist_encoding(ae_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7aa2dcb-3add-4638-8ee6-f4ed4388386e",
      "metadata": {
        "id": "e7aa2dcb-3add-4638-8ee6-f4ed4388386e"
      },
      "source": [
        "The scatter plots of the encoded data above demonstrates that the encoder does a decent job at separating digites of different classes despite the extremely reduced dimensionality of the latent space. That said, the latent space is not necessarily meaningful. There are plenty of regions that are empty and the encoding does not follow a predictable structure making it difficult to use this latent space to generate samples.\n",
        "\n",
        "To bring structure to the latent space, we need to enforce some sort of constraint over the structure of the latent space. This is where **Variational Autoencoders** (VAE) come in."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7897f647-9ebf-470e-a06a-90ccc85054e9",
      "metadata": {
        "id": "7897f647-9ebf-470e-a06a-90ccc85054e9"
      },
      "source": [
        "Let's save the model. All we have to do is save the so-called `state_dict` of the trained model, i.e. the model weights/parameters. We can then reload the model by simply loading it onto the model (as long as the model architecture is unchanged). This is the only thing we really need to save as long as we keep track of the architecture of the network we built. More details on saving models can be found in the [PyTorch documentation](https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_models_for_inference.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e887cb11-8017-41c9-a4b4-1065819f4e4b",
      "metadata": {
        "id": "e887cb11-8017-41c9-a4b4-1065819f4e4b"
      },
      "outputs": [],
      "source": [
        "saved_file_name = \"mnist_ae_weights.pth\"  # the extension does not matter\n",
        "# Save learned parameters\n",
        "torch.save(ae_model.state_dict(), saved_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "617ce950-c674-4fec-97ff-9a5d55be584a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "617ce950-c674-4fec-97ff-9a5d55be584a",
        "outputId": "b135352e-18d0-4ae8-b1b2-be1fc5b4a208"
      },
      "outputs": [],
      "source": [
        "# Load model parameters\n",
        "reloaded_ae_model = AutoEncoder(encoder=Encoder(), decoder=Decoder()).to(device)\n",
        "reloaded_ae_model.load_state_dict(torch.load(saved_file_name))\n",
        "reloaded_ae_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "479b7775-4fb3-4cc7-b4ef-d214a8211316",
      "metadata": {
        "id": "479b7775-4fb3-4cc7-b4ef-d214a8211316"
      },
      "source": [
        "## 3. Variational autoencoder\n",
        "We have built a vanilla autoencoder. As we've seen, its latent space does not look amazing. Let's see if how the latent space of an equivalent variational autoencoder looks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a0d7381-76c5-418f-a28c-6170d67cbb33",
      "metadata": {
        "id": "7a0d7381-76c5-418f-a28c-6170d67cbb33"
      },
      "outputs": [],
      "source": [
        "class VariationalAutoEncoder(AutoEncoder):\n",
        "    def __init__(self, encoder, decoder, latent_dimensionality):\n",
        "        super(VariationalAutoEncoder, self).__init__(encoder, decoder)\n",
        "\n",
        "        self.latent_dimensionality = latent_dimensionality\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        mu = x[:, : self.latent_dimensionality]\n",
        "        log_variance = x[:, self.latent_dimensionality :]\n",
        "        # sample from gaussian dis\n",
        "        # project to log sigma^2 so we dont get negative values.\n",
        "        variance = torch.exp(log_variance) #get rid of the negatives?\n",
        "        eps = torch.randn_like(variance) #sample from a normal distribution\n",
        "        sample = mu + torch.sqrt(variance) * eps #randomly scale, close to mu but not mu.\n",
        "\n",
        "        x = self.decoder(sample)\n",
        "        return x, mu, log_variance #we need mu and log_variance to compute KLdivergence.\n",
        "\n",
        "    def kullback_leibler_divergence(self, mu, log_variance):\n",
        "      #commonly derive expression\n",
        "        return 0.5 * torch.mean(mu ** 2 + torch.exp(log_variance) - (1 + log_variance))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7491a6ca-ed55-49d4-8e8b-d9002a43dc44",
      "metadata": {
        "id": "7491a6ca-ed55-49d4-8e8b-d9002a43dc44"
      },
      "outputs": [],
      "source": [
        "vae_model = VariationalAutoEncoder(\n",
        "    Encoder(latent_dimensionality=4),\n",
        "    Decoder(latent_dimensionality=2),\n",
        "    latent_dimensionality=2,\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "436f941f-c26c-4d97-9964-448faa36f3c4",
      "metadata": {
        "id": "436f941f-c26c-4d97-9964-448faa36f3c4"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(vae_model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "737917cd-47e2-4a93-a101-43fa15ba99f4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b6133289a216451e9780fb1d27ba03c8",
            "b19e99107c6146deb9a0dc10be5e6737",
            "fb06cdd7f4b04f9086ff41cf4e162c94",
            "293ac590c7ae4c4c983f2ae84a7b2e0b",
            "b0f69ed946a14fd19b2d02026fd19742",
            "383e882106f747c49d8963a7b3998027",
            "a73fd0bde28d407da73776bbb407fc81",
            "4f66ef7661ff457aa710d5d2851dad6a",
            "3d9839eaa1e44a18b373d893c98d8221",
            "932348d7172542868874f05f96da5ea5",
            "fa41adc6270b4f32a4749f7709d2a0bb"
          ]
        },
        "id": "737917cd-47e2-4a93-a101-43fa15ba99f4",
        "outputId": "728c1298-d235-4743-db75-1be97201c61f"
      },
      "outputs": [],
      "source": [
        "n_epochs = 20\n",
        "reporting_frequency = 200\n",
        "for i in tqdm(range(1, n_epochs + 1)):\n",
        "\n",
        "    # training\n",
        "    training_losses = []\n",
        "    for j, (digit_images, _) in enumerate(training_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        digit_images = digit_images.to(device)\n",
        "        reconstructed, mu, log_variance = vae_model(digit_images)\n",
        "        reconstruction_loss = vae_model.reconstruction_loss(reconstructed, digit_images)\n",
        "        kl_divergence_loss = vae_model.kullback_leibler_divergence(mu, log_variance)\n",
        "\n",
        "        #loss has 2 terms -> reconstruction loss and kl_divergence_loss\n",
        "        #scale 0.01 so it't not too big/small so it includes the kl_divergence, but not too much that interrupt the workflow.\n",
        "        loss = reconstruction_loss + 0.01 * kl_divergence_loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        training_losses.append(loss.detach().cpu().numpy())\n",
        "        if (j + 1) % reporting_frequency == 0:\n",
        "            print(\n",
        "                \"Epoch {0} | Mean training loss after {1} batches: {2:.3f}\".format(\n",
        "                    i, j + 1, np.mean(training_losses)\n",
        "                )\n",
        "            )\n",
        "\n",
        "    # validation\n",
        "    validation_losses = []\n",
        "    for j, (digit_images, _) in enumerate(validation_dataloader):\n",
        "        digit_images = digit_images.to(device)\n",
        "        reconstructed, _, _ = vae_model(digit_images)\n",
        "        loss = vae_model.reconstruction_loss(reconstructed, digit_images)\n",
        "\n",
        "        validation_losses.append(loss.detach().cpu().numpy())\n",
        "\n",
        "    print(\n",
        "        \"Epoch {0} | Mean training total loss: {1:.3f} | Mean validation reconstruction loss: {2:.3f}\".format(\n",
        "            i, np.mean(training_losses), np.mean(validation_losses)\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fd5e3f1-3dd7-45c9-b834-3997debadace",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "1fd5e3f1-3dd7-45c9-b834-3997debadace",
        "outputId": "591f0c17-cec2-484a-e779-9e78019e9653"
      },
      "outputs": [],
      "source": [
        "plot_mnist_encoding(vae_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "763968a4-b871-450e-9da9-851ea8f2289a",
      "metadata": {
        "id": "763968a4-b871-450e-9da9-851ea8f2289a"
      },
      "source": [
        "The visualization above, especially in constrast to the identical one done for the regular autoencoder shows the impact of the Kullback-Leibler divergence on the distribution of the encoded points in the latent space."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5282699-b818-42f6-9534-c55a2b7556e5",
      "metadata": {
        "id": "d5282699-b818-42f6-9534-c55a2b7556e5"
      },
      "source": [
        "Let's visualize how the entire latent space decodes at once. How can we do that? Well, we can simply sample the latent space with a grid at a given resolution, decode each point into an image using a decoder and plot the mosaic of decoded images to get a full picture of the latent space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1861c7d-65f9-444e-9f8c-0bbd62ca55b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "f1861c7d-65f9-444e-9f8c-0bbd62ca55b1",
        "outputId": "7d74abf6-0c39-4f9a-9b0e-811910eb79d2"
      },
      "outputs": [],
      "source": [
        "n = 10  # number of images per rows columns\n",
        "\n",
        "# grid sampling in latent space\n",
        "z = np.linspace(-3, 3, n)\n",
        "Z1, Z2 = np.meshgrid(z, z)\n",
        "Z = np.vstack([Z1.flatten(), Z2.flatten()]).T\n",
        "\n",
        "# decode samples\n",
        "\n",
        "ims = vae_model.decoder(torch.tensor(Z).float().to(device).unsqueeze(0)).detach().cpu()\n",
        "\n",
        "# plot\n",
        "fig, axes = plt.subplots(n, n, figsize=(7, 7))\n",
        "fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
        "count = 0\n",
        "for k in range(0, ims.shape[0]):\n",
        "    i, j = int(count / n), int(count % n)\n",
        "    axes[i, j].imshow(ims[k, 0, :, :])\n",
        "    axes[i, j].axis(\"off\")\n",
        "    count += 1\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51eb9c01-2151-488c-91d9-da8354b75aa8",
      "metadata": {
        "id": "51eb9c01-2151-488c-91d9-da8354b75aa8"
      },
      "source": [
        "We sampled the latent space with a simple evenly-space grid. Is that how we should sample the latent space? Not quite. Indeed, with a VAE, we are trying to constrain our encoded data to fit a normal distribution. That is, we are forcing the encoding to be denser around $[0,0]$ and sparser further away from the origin. If we sample with an evenly-spaced grid, we are not sampling those denser regions as much as we should. The visualization above illustrates this as there is little variation in the decoded samples but at the very center of the mosaic. This is where we'd like to have a denser sampling\n",
        "\n",
        "It turns out there is a simple solution to this issue: sample in probability space. Instead of sampling at $x$ where $[x=-3,x=-2,x=-1,...]$, we'll sample at $x$ where $[p(x)=0.05, p(x)=0.1, p(x)=0.15,...]$ where $p(x)$ is the cumulative density function (cdf) of the normal distribution (i.e. the probability that a sample from the normal distribution would be smaller than $x$). To find these sampling locations, we can use the inverse of the cdf, i.e. the percent point function (ppf) or quantile function, whihc is implemented in `scipy`.\n",
        "\n",
        "This may seem a little esoteric, so let's plot things to shed some light on what this type of sampling translates to. The plot below shows the distribution of points sampled on an evenly-spaced grid and on a grid that is evenly-spaced in probability terms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1d997f6-d92e-46c3-a42f-f222880ffb16",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "c1d997f6-d92e-46c3-a42f-f222880ffb16",
        "outputId": "87bde502-227b-4727-8c41-0ecbd1a7a873"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7, 7))\n",
        "\n",
        "n = 20  # sampling resolution\n",
        "# evenly-spaced grid sampling in latent space\n",
        "z = np.linspace(-3, 3, n)\n",
        "Z1, Z2 = np.meshgrid(z, z)\n",
        "Z = np.vstack([Z1.flatten(), Z2.flatten()]).T\n",
        "\n",
        "plt.scatter(Z[:, 0], Z[:, 1], s=50, c=\"black\", label=\"Grid sampling\")\n",
        "\n",
        "z = scipy.stats.norm.ppf(np.linspace(0.005, 0.995, n))\n",
        "Z1, Z2 = np.meshgrid(z, z)\n",
        "Z = np.vstack([Z1.flatten(), Z2.flatten()]).T\n",
        "\n",
        "plt.scatter(Z[:, 0], Z[:, 1], marker=\"+\", s=100, c=\"red\", label=\"PPF sampling\")\n",
        "plt.legend(fontsize=10, bbox_to_anchor=(1, 0.5))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59c98fb5-84ec-4119-9ce5-5ffa70d42b84",
      "metadata": {
        "id": "59c98fb5-84ec-4119-9ce5-5ffa70d42b84"
      },
      "source": [
        "Now, let's use this sampling scheme on the MNIST VAE (with a higher resolution) and see what we get."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78145753-17cd-41fd-b416-fa4a728b14f0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "id": "78145753-17cd-41fd-b416-fa4a728b14f0",
        "outputId": "c1b48d4b-4f8e-4aeb-879f-c0fb97374dcf"
      },
      "outputs": [],
      "source": [
        "n = 20  # number of images per rows columns\n",
        "\n",
        "# grid sampling in latent space\n",
        "z = scipy.stats.norm.ppf(np.linspace(0.005, 0.995, n))\n",
        "Z1, Z2 = np.meshgrid(z, z)\n",
        "Z = np.vstack([Z1.flatten(), Z2.flatten()]).T\n",
        "\n",
        "# decode samples\n",
        "ims = vae_model.decoder(torch.tensor(Z).float().to(device).unsqueeze(0)).detach().cpu()\n",
        "\n",
        "# plot\n",
        "fig, axes = plt.subplots(n, n, figsize=(7, 7))\n",
        "fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
        "count = 0\n",
        "for k in range(0, ims.shape[0]):\n",
        "    i, j = int(count / n), int(count % n)\n",
        "    axes[i, j].imshow(ims[k, 0, :, :])\n",
        "    axes[i, j].axis(\"off\")\n",
        "    count += 1\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "894f3358-8a59-4d81-8317-2184cbbad2da",
      "metadata": {
        "id": "894f3358-8a59-4d81-8317-2184cbbad2da"
      },
      "source": [
        "We're now getting much more detail about what's happening closer to the origin, where most of the action truly is. This plot essentially provides a good snapshot of the spectrum of images the decoder is able to generate/reproduce.\n",
        "\n",
        "What is pretty remarkable is the ability of the decoder to smoothly interpolate between different digits, for example to produce 1s that almost look like 9s or vice-versa.That is the power of latent variable modeling. And now, imagine those were pictures of designs. You could use the same technique to compress a design space and provide a reduced map of designs just like this. We'll discuss this type of approach this week.\n",
        "\n",
        "Clearly, it cannot reproduce everything, as we would expect. Such heavy compression is almost inevitably lossy, and in this case, a latent dimensionality of 2 is probably too small. We may be able to push things further by increasing the complexity of the model but we would some limit because we just cannot fit an infinite amount of variation in that small space.\n",
        "\n",
        "Reducing the size of your latent space almost inevitably means reducing its representational power, but a smaller latent space is also easier to explore, visualize, and exploit for design generation, so this trade-off is often worth it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bf3dac6-8cd5-4c23-b4c5-5196267102a6",
      "metadata": {
        "id": "9bf3dac6-8cd5-4c23-b4c5-5196267102a6"
      },
      "source": [
        "Let's build the same plot for the autoencoder to demonstrate the difference between the latent spaces produded. Since they encoding is not constrained to meet any sort of distribution, we'll use regular grid sampling across the bounding box of encoded validation samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7902b5d-f3a1-4c58-b187-a3fb47fd3668",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "a7902b5d-f3a1-4c58-b187-a3fb47fd3668",
        "outputId": "c4eebbd0-7998-4781-f8c9-0a2ef18c9306"
      },
      "outputs": [],
      "source": [
        "# get bounding box\n",
        "ae_encodings, _ = get_all_encodings(ae_model, validation_dataloader)\n",
        "x_min, y_min = np.min(ae_encodings, axis=0)\n",
        "x_max, y_max = np.max(ae_encodings, axis=0)\n",
        "\n",
        "\n",
        "n = 20  # number of images per rows columns\n",
        "\n",
        "# grid sampling in latent space\n",
        "z1 = np.linspace(0, 1, n) * (x_max - x_min) + x_min\n",
        "z2 = np.linspace(0, 1, n) * (y_max - y_min) + y_min\n",
        "Z1, Z2 = np.meshgrid(z1, z2)\n",
        "Z = np.vstack([Z1.flatten(), Z2.flatten()]).T\n",
        "\n",
        "# decode samples\n",
        "ims = ae_model.decoder(torch.tensor(Z).float().to(device).unsqueeze(0)).detach().cpu()\n",
        "\n",
        "# plot\n",
        "fig, axes = plt.subplots(n, n, figsize=(7, 7))\n",
        "fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
        "count = 0\n",
        "for k in range(0, ims.shape[0]):\n",
        "    i, j = int(count / n), int(count % n)\n",
        "    axes[i, j].imshow(ims[k, 0, :, :])\n",
        "    axes[i, j].axis(\"off\")\n",
        "    count += 1\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff4ded48-0568-43b4-b8a2-b835f33ab230",
      "metadata": {
        "id": "ff4ded48-0568-43b4-b8a2-b835f33ab230"
      },
      "source": [
        "Clearly, the latent space is not as \"nice\" as the one produced by the VAE, i.e. most of the space contains little variations and/or garbage."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00c79321b1b8498bbb32d5990a0549f0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cf4dd65a1634d19a0995ae9f48ade68": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00c79321b1b8498bbb32d5990a0549f0",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_54dce2fa77ef4a6da5440107e8a44ce2",
            "value": 20
          }
        },
        "293ac590c7ae4c4c983f2ae84a7b2e0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_932348d7172542868874f05f96da5ea5",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_fa41adc6270b4f32a4749f7709d2a0bb",
            "value": " 20/20 [05:28&lt;00:00, 17.31s/it]"
          }
        },
        "383e882106f747c49d8963a7b3998027": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "388e23336c2c4210b05993d9468d8c24": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d9839eaa1e44a18b373d893c98d8221": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4f66ef7661ff457aa710d5d2851dad6a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54dce2fa77ef4a6da5440107e8a44ce2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "664949285a69435e8ee81a22eabd1929": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c9dbcb3adf6a49c6a81419782e2f425a",
              "IPY_MODEL_1cf4dd65a1634d19a0995ae9f48ade68",
              "IPY_MODEL_bf6d2e5da0134f0d84eca2976e0c01b6"
            ],
            "layout": "IPY_MODEL_90b9b815b18e4e99957e970f44502440"
          }
        },
        "8bb5e8fdc18048b78de2fa49e53afad7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90b9b815b18e4e99957e970f44502440": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92cb109b42c54c3e8774407e2fc71e72": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "932348d7172542868874f05f96da5ea5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a73fd0bde28d407da73776bbb407fc81": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0f69ed946a14fd19b2d02026fd19742": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b19e99107c6146deb9a0dc10be5e6737": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_383e882106f747c49d8963a7b3998027",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a73fd0bde28d407da73776bbb407fc81",
            "value": "100%"
          }
        },
        "b6133289a216451e9780fb1d27ba03c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b19e99107c6146deb9a0dc10be5e6737",
              "IPY_MODEL_fb06cdd7f4b04f9086ff41cf4e162c94",
              "IPY_MODEL_293ac590c7ae4c4c983f2ae84a7b2e0b"
            ],
            "layout": "IPY_MODEL_b0f69ed946a14fd19b2d02026fd19742"
          }
        },
        "bf6d2e5da0134f0d84eca2976e0c01b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa002478f4e94264bbdef5694022bd3d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_92cb109b42c54c3e8774407e2fc71e72",
            "value": " 20/20 [05:56&lt;00:00, 18.20s/it]"
          }
        },
        "c9dbcb3adf6a49c6a81419782e2f425a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8bb5e8fdc18048b78de2fa49e53afad7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_388e23336c2c4210b05993d9468d8c24",
            "value": "100%"
          }
        },
        "fa002478f4e94264bbdef5694022bd3d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa41adc6270b4f32a4749f7709d2a0bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb06cdd7f4b04f9086ff41cf4e162c94": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f66ef7661ff457aa710d5d2851dad6a",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3d9839eaa1e44a18b373d893c98d8221",
            "value": 20
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
